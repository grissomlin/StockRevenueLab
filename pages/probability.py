import streamlit as st
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import urllib.parse
import plotly.graph_objects as go

# ========== 1. é é¢é…ç½® ==========
st.set_page_config(page_title="æ©Ÿç‡ç ”ç©¶å®¤ 2.0 | StockRevenueLab", layout="wide")

# ========== 2. å®‰å…¨è³‡æ–™åº«é€£ç·š ==========
@st.cache_resource
def get_engine():
    try:
        DB_PASSWORD = st.secrets["DB_PASSWORD"]
        PROJECT_REF = st.secrets["PROJECT_REF"]
        POOLER_HOST = st.secrets["POOLER_HOST"]
        encoded_password = urllib.parse.quote_plus(DB_PASSWORD)
        connection_string = f"postgresql://postgres.{PROJECT_REF}:{encoded_password}@{POOLER_HOST}:5432/postgres?sslmode=require"
        return create_engine(connection_string)
    except Exception:
        st.error("âŒ è³‡æ–™åº«é€£ç·šå¤±æ•—ï¼Œè«‹æª¢æŸ¥ Secrets è¨­å®š")
        st.stop()

# ========== 3. æ–°å¢ï¼šç²å–å‰å¾Œå¹´åº¦æ¯”è¼ƒæ•¸æ“šï¼ˆåŒ…å«ä¸­ä½æ•¸ï¼‰ ==========
@st.cache_data(ttl=3600)
def fetch_multi_year_data(stock_list, target_year):
    """ç²å–æŒ‡å®šè‚¡ç¥¨åœ¨å‰å¾Œå¹´åº¦çš„è¡¨ç¾ï¼ˆåŒ…å«ä¸­ä½æ•¸ï¼‰"""
    if not stock_list:
        return pd.DataFrame()
    
    engine = get_engine()
    stock_ids = ','.join([f"'{id}'" for id in stock_list])
    
    query = f"""
    WITH years_data AS (
        SELECT 
            SPLIT_PART(symbol, '.', 1) as stock_id,
            year,
            ((year_close - year_open) / year_open) * 100 as annual_return
        FROM stock_annual_k
        WHERE SPLIT_PART(symbol, '.', 1) IN ({stock_ids})
            AND year::integer BETWEEN {int(target_year)-2} AND {int(target_year)+1}
    )
    SELECT * FROM years_data;
    """
    
    with engine.connect() as conn:
        return pd.read_sql_query(text(query), conn)

# ========== 4. æ•¸æ“šæŠ“å–å¼•æ“ (ç²¾ç¢ºå°é½Šå¹´åº¦å ±è¡¨) ==========
@st.cache_data(ttl=3600)
def fetch_prob_data(year, metric_col, low, high):
    engine = get_engine()
    minguo_year = int(year) - 1911
    prev_minguo_year = minguo_year - 1
    
    query = f"""
    WITH hit_table AS (
        SELECT stock_id, COUNT(*) as hits 
        FROM monthly_revenue 
        WHERE (
            report_month = '{prev_minguo_year}_12' 
            OR (report_month LIKE '{minguo_year}_%' AND report_month <= '{minguo_year}_11')
        )
        AND {metric_col} >= {low} AND {metric_col} < {high}
        GROUP BY stock_id
    ),
    perf_table AS (
        SELECT SPLIT_PART(symbol, '.', 1) as stock_id, 
                ((year_close - year_open) / year_open)*100 as ret
        FROM stock_annual_k WHERE year = '{year}'
    )
    SELECT h.hits as "çˆ†ç™¼æ¬¡æ•¸", COUNT(*) as "è‚¡ç¥¨æª”æ•¸",
           ROUND(AVG(p.ret)::numeric, 1) as "å¹³å‡å¹´åº¦æ¼²å¹…%",
           ROUND(MEDIAN(p.ret)::numeric, 1) as "ä¸­ä½æ•¸æ¼²å¹…%",
           ROUND((COUNT(*) FILTER (WHERE p.ret > 20) * 100.0 / COUNT(*))::numeric, 1) as "å‹ç‡(>20%)",
           ROUND((COUNT(*) FILTER (WHERE p.ret > 100) * 100.0 / COUNT(*))::numeric, 1) as "ç¿»å€ç‡(>100%)",
           ROUND(MIN(p.ret)::numeric, 1) as "æœ€ä½æ¼²å¹…%",
           ROUND(MAX(p.ret)::numeric, 1) as "æœ€é«˜æ¼²å¹…%",
           ROUND(STDDEV(p.ret)::numeric, 1) as "æ¨™æº–å·®%"
    FROM hit_table h JOIN perf_table p ON h.stock_id = p.stock_id
    GROUP BY h.hits ORDER BY h.hits DESC;
    """
    with engine.connect() as conn:
        return pd.read_sql_query(text(query), conn)

# ========== 5. æ–°å¢ï¼šè¨ˆç®—æœŸæœ›å€¼æŒ‡æ¨™ ==========
def calculate_expected_value(df):
    """è¨ˆç®—æœŸæœ›å€¼ç›¸é—œæŒ‡æ¨™"""
    results = []
    for _, row in df.iterrows():
        hits = row["çˆ†ç™¼æ¬¡æ•¸"]
        count = row["è‚¡ç¥¨æª”æ•¸"]
        avg_return = row["å¹³å‡å¹´åº¦æ¼²å¹…%"]
        median_return = row["ä¸­ä½æ•¸æ¼²å¹…%"]
        win_rate = row["å‹ç‡(>20%)"] / 100
        
        # ç°¡å–®æœŸæœ›å€¼ = å¹³å‡å ±é…¬ * è‚¡ç¥¨æª”æ•¸ï¼ˆæ¬Šé‡ï¼‰
        expected_value = avg_return * count
        
        # é¢¨éšªèª¿æ•´å¾ŒæœŸæœ›å€¼ï¼ˆè€ƒæ…®æ¨™æº–å·®ï¼‰
        risk_adjusted = avg_return / max(row["æ¨™æº–å·®%"], 1)
        
        # æˆåŠŸç‡èª¿æ•´æœŸæœ›å€¼
        success_adjusted = avg_return * win_rate
        
        # å¹³å‡æ•¸èˆ‡ä¸­ä½æ•¸å·®ç•°
        mean_median_diff = avg_return - median_return
        
        results.append({
            "çˆ†ç™¼æ¬¡æ•¸": hits,
            "è‚¡ç¥¨æª”æ•¸": count,
            "å¹³å‡å¹´åº¦æ¼²å¹…%": avg_return,
            "ä¸­ä½æ•¸æ¼²å¹…%": median_return,
            "å¹³å‡-ä¸­ä½å·®": round(mean_median_diff, 1),
            "å‹ç‡(>20%)": row["å‹ç‡(>20%)"],
            "ç¿»å€ç‡(>100%)": row["ç¿»å€ç‡(>100%)"],
            "æœŸæœ›å€¼åˆ†æ•¸": round(expected_value / 100, 2),
            "é¢¨éšªèª¿æ•´åˆ†æ•¸": round(risk_adjusted, 2),
            "æˆåŠŸç‡åˆ†æ•¸": round(success_adjusted, 2),
            "ç¶œåˆè©•åˆ†": round((expected_value/100 + risk_adjusted + success_adjusted) / 3, 2)
        })
    
    return pd.DataFrame(results)

# ========== 6. UI ä»‹é¢è¨­è¨ˆ ==========
st.title("ğŸ² ç‡Ÿæ”¶çˆ†ç™¼èˆ‡å¹´åº¦å ±é…¬æ©Ÿç‡åˆ†æ 2.0")
st.markdown("""
**ç ”ç©¶ç›®æ¨™**ï¼šåˆ†ææœˆå¢ç‡(MoM)æˆ–å¹´å¢ç‡(YoY)å‡ºç¾ç‰¹å®šæ¬¡æ•¸èˆ‡è‚¡åƒ¹å¹´åº¦å ±é…¬çš„é—œä¿‚

**ç ”ç©¶æœŸé–“**ï¼šå‰ä¸€å¹´12æœˆ ~ ç›®æ¨™å¹´11æœˆï¼ˆå…±12ä»½æœˆç‡Ÿæ”¶å ±å‘Šï¼‰
**è‚¡åƒ¹è¨ˆç®—**ï¼šç›®æ¨™å¹´åº¦å…¨å¹´æ¼²è·Œå¹…ï¼ˆå¹´Kç·šé–‹ç›¤åˆ°æ”¶ç›¤ï¼‰
""")

with st.sidebar:
    st.header("ğŸ”¬ ç ”ç©¶åƒæ•¸è¨­å®š")
    target_year = st.sidebar.selectbox("ç›®æ¨™å¹´åº¦", [str(y) for y in range(2025, 2019, -1)], index=1)
    
    study_metric = st.selectbox(
        "ç ”ç©¶æŒ‡æ¨™",
        ["yoy_pct", "mom_pct"],
        format_func=lambda x: "å¹´å¢ç‡(YoY)" if x == "yoy_pct" else "æœˆå¢ç‡(MoM)",
        index=0,
        help="å¹´å¢ç‡ï¼šèˆ‡å»å¹´åŒæœŸæ¯”è¼ƒï¼›æœˆå¢ç‡ï¼šèˆ‡ä¸Šæœˆæ¯”è¼ƒ"
    )
    
    metric_name = "å¹´å¢ç‡(YoY)" if study_metric == "yoy_pct" else "æœˆå¢ç‡(MoM)"
    
    growth_range = st.select_slider(
        f"è¨­å®š{metric_name}çˆ†ç™¼å€é–“ (%)", 
        options=[-50, 0, 20, 50, 100, 150, 200, 300, 500, 1000], 
        value=(100, 1000)
    )
    
    st.markdown("---")
    st.markdown("### ğŸ“Š åˆ†æé¸é …")
    show_advanced = st.checkbox("é¡¯ç¤ºé€²éšåˆ†æ", value=True)
    show_multi_year = st.checkbox("é¡¯ç¤ºå‰å¾Œå¹´åº¦æ¯”è¼ƒ", value=True)
    show_expected_value = st.checkbox("è¨ˆç®—æœŸæœ›å€¼è©•åˆ†", value=True)

# ç²å–ä¸»è¦æ•¸æ“š
df_prob = fetch_prob_data(target_year, study_metric, growth_range[0], growth_range[1])

if not df_prob.empty:
    # ========== A. æ ¸å¿ƒæ•¸æ“šé¡¯ç¤ºå€ ==========
    st.subheader(f"ğŸ“Š {target_year}å¹´ï¼š{metric_name}é”æ¨™æ¬¡æ•¸ vs å¹´åº¦å ±é…¬çµ±è¨ˆ")
    
    # é¡¯ç¤ºåŸºæœ¬çµ±è¨ˆ
    total_stocks = df_prob["è‚¡ç¥¨æª”æ•¸"].sum()
    st.metric("ç¸½æ¨£æœ¬è‚¡ç¥¨æ•¸", f"{total_stocks} æª”")
    
    # é¡¯ç¤ºåŸå§‹è¡¨æ ¼
    display_cols = ["çˆ†ç™¼æ¬¡æ•¸", "è‚¡ç¥¨æª”æ•¸", "å¹³å‡å¹´åº¦æ¼²å¹…%", "ä¸­ä½æ•¸æ¼²å¹…%", 
                    "å‹ç‡(>20%)", "ç¿»å€ç‡(>100%)", "æ¨™æº–å·®%"]
    
    st.dataframe(df_prob[display_cols].style.format({
        "å¹³å‡å¹´åº¦æ¼²å¹…%": "{:.1f}%",
        "ä¸­ä½æ•¸æ¼²å¹…%": "{:.1f}%",
        "å‹ç‡(>20%)": "{:.1f}%", 
        "ç¿»å€ç‡(>100%)": "{:.1f}%",
        "æ¨™æº–å·®%": "{:.1f}%"
    }), use_container_width=True)
    
    # ========== B. è¦–è¦ºåŒ–åˆ†æ ==========
    if show_advanced:
        col1, col2 = st.columns(2)
        
        with col1:
            # çˆ†ç™¼æ¬¡æ•¸ vs å¹³å‡å ±é…¬èˆ‡ä¸­ä½æ•¸
            fig1 = go.Figure()
            fig1.add_trace(go.Bar(
                x=df_prob["çˆ†ç™¼æ¬¡æ•¸"],
                y=df_prob["å¹³å‡å¹´åº¦æ¼²å¹…%"],
                name='å¹³å‡å¹´åº¦æ¼²å¹…%',
                marker_color='lightblue'
            ))
            fig1.add_trace(go.Scatter(
                x=df_prob["çˆ†ç™¼æ¬¡æ•¸"],
                y=df_prob["ä¸­ä½æ•¸æ¼²å¹…%"],
                name='ä¸­ä½æ•¸æ¼²å¹…%',
                mode='lines+markers',
                line=dict(color='darkblue', width=2)
            ))
            fig1.update_layout(
                title=f"{metric_name}çˆ†ç™¼æ¬¡æ•¸ vs å¹´åº¦è¡¨ç¾",
                yaxis_title='æ¼²å¹… %',
                height=400
            )
            st.plotly_chart(fig1, use_container_width=True)
        
        with col2:
            # å¹³å‡æ•¸ vs ä¸­ä½æ•¸å·®ç•°
            df_prob['å¹³å‡-ä¸­ä½å·®'] = df_prob['å¹³å‡å¹´åº¦æ¼²å¹…%'] - df_prob['ä¸­ä½æ•¸æ¼²å¹…%']
            
            fig2 = go.Figure()
            fig2.add_trace(go.Bar(
                x=df_prob["çˆ†ç™¼æ¬¡æ•¸"],
                y=df_prob["å¹³å‡-ä¸­ä½å·®"],
                name='å¹³å‡-ä¸­ä½æ•¸å·®ç•°',
                marker_color='coral',
                text=df_prob["å¹³å‡-ä¸­ä½å·®"].round(1),
                textposition='outside'
            ))
            fig2.update_layout(
                title="å¹³å‡æ•¸èˆ‡ä¸­ä½æ•¸å·®ç•°åˆ†æ",
                yaxis_title="å·®ç•° %",
                height=400
            )
            st.plotly_chart(fig2, use_container_width=True)
            
            # è§£é‡‹å·®ç•°
            pos_diff_count = (df_prob['å¹³å‡-ä¸­ä½å·®'] > 0).sum()
            pos_diff_percent = pos_diff_count / len(df_prob) * 100
            
            st.info(f"""
            **å¹³å‡æ•¸èˆ‡ä¸­ä½æ•¸å·®ç•°åˆ†æ**ï¼š
            - {pos_diff_count}/{len(df_prob)} å€‹å€é–“({pos_diff_percent:.1f}%) å¹³å‡æ•¸ > ä¸­ä½æ•¸
            - **è¡¨ç¤ºå¤šæ•¸å€é–“å­˜åœ¨å³ååˆ†ä½ˆ**ï¼šå°‘æ•¸è‚¡ç¥¨æ¼²å¹…æ¥µé«˜ï¼Œæ‹‰é«˜äº†å¹³å‡å€¼
            - ç•¶å·®ç•°è¶Šå¤§ï¼Œä»£è¡¨è©²çˆ†ç™¼æ¬¡æ•¸å€é–“çš„**å³å°¾æ•ˆæ‡‰**è¶Šæ˜é¡¯
            """)
    
    # ========== C. æœŸæœ›å€¼åˆ†æ ==========
    if show_expected_value and len(df_prob) > 1:
        st.subheader("ğŸ¯ æœŸæœ›å€¼èˆ‡ç¶œåˆè©•åˆ†åˆ†æ")
        
        # è¨ˆç®—æœŸæœ›å€¼æŒ‡æ¨™
        expected_df = calculate_expected_value(df_prob)
        
        # æ‰¾å‡ºæœ€ä½³å€é–“
        if 'ç¶œåˆè©•åˆ†' in expected_df.columns:
            best_idx = expected_df["ç¶œåˆè©•åˆ†"].idxmax()
            best_hits = expected_df.loc[best_idx, "çˆ†ç™¼æ¬¡æ•¸"]
            best_score = expected_df.loc[best_idx, "ç¶œåˆè©•åˆ†"]
        else:
            best_hits = df_prob.loc[0, "çˆ†ç™¼æ¬¡æ•¸"]
            best_score = 0
        
        col_a, col_b, col_c = st.columns(3)
        col_a.metric("æœ€ä½³çˆ†ç™¼æ¬¡æ•¸", f"{best_hits} æ¬¡")
        col_b.metric("ç¶œåˆè©•åˆ†", f"{best_score:.2f}")
        col_c.metric("è©²å€é–“æ¨£æœ¬æ•¸", f"{int(expected_df.loc[best_idx if 'best_idx' in locals() else 0, 'è‚¡ç¥¨æª”æ•¸'])} æª”")
        
        # é¡¯ç¤ºæœŸæœ›å€¼è¡¨æ ¼
        st.dataframe(expected_df.style.format({
            "å¹³å‡å¹´åº¦æ¼²å¹…%": "{:.1f}",
            "ä¸­ä½æ•¸æ¼²å¹…%": "{:.1f}",
            "å¹³å‡-ä¸­ä½å·®": "{:.1f}",
            "æœŸæœ›å€¼åˆ†æ•¸": "{:.2f}",
            "é¢¨éšªèª¿æ•´åˆ†æ•¸": "{:.2f}",
            "æˆåŠŸç‡åˆ†æ•¸": "{:.2f}",
            "ç¶œåˆè©•åˆ†": "{:.2f}"
        }).highlight_max(subset=["ç¶œåˆè©•åˆ†"], color='lightgreen'), 
        use_container_width=True)
    
    # ========== D. AI åˆ†æåŠ©æ‰‹å€ (æ”¹é€²ç‰ˆ) ==========
    st.markdown("---")
    st.subheader("ğŸ¤– AI æ·±åº¦ç­–ç•¥è¨ºæ–·")
    
    # å»ºæ§‹Markdownè¡¨æ ¼
    header = "| " + " | ".join(df_prob.columns) + " |"
    sep = "| " + " | ".join(["---"] * len(df_prob.columns)) + " |"
    rows = ["| " + " | ".join(map(str, row.values)) + " |" for _, row in df_prob.iterrows()]
    table_md = "\n".join([header, sep] + rows)
    
    # å»ºæ§‹å®Œæ•´çš„æç¤ºè©
    prompt_text = f"""
# {target_year}å¹´å°è‚¡ç‡Ÿæ”¶çˆ†ç™¼æ¬¡æ•¸èˆ‡å¹´åº¦å ±é…¬é—œè¯åˆ†æ

## ç ”ç©¶è¨­å®š
- **åˆ†æå¹´åº¦**: {target_year}å¹´
- **ç ”ç©¶æŒ‡æ¨™**: {metric_name}
- **çˆ†ç™¼é–€æª»**: {growth_range[0]}% è‡³ {growth_range[1]}%
- **ç ”ç©¶æœŸé–“**: å‰ä¸€å¹´12æœˆåˆ°{target_year}å¹´11æœˆï¼ˆ12å€‹æœˆä»½ï¼‰
- **è‚¡åƒ¹è¨ˆç®—**: {target_year}å¹´åº¦æ¼²è·Œå¹…ï¼ˆå¹´Kç·šï¼‰

## å®Œæ•´çµ±è¨ˆæ•¸æ“š
{table_md}

## é—œéµè§€å¯Ÿé»ï¼ˆå¾æ•¸æ“šä¸­ç™¼ç¾ï¼‰
1. **æ¥µç«¯å€¼åˆ†æ**: 
   - çˆ†ç™¼12æ¬¡çš„æœ‰2æª”è‚¡ç¥¨ï¼Œå¹³å‡æ¼²å¹…221.8%ï¼Œå‹ç‡100%ï¼Œç¿»å€ç‡100%
   - çˆ†ç™¼11æ¬¡çš„åƒ…1æª”ï¼Œæ¼²å¹…-24.4%ï¼Œå…¨éƒ¨è™§æ

2. **æ¨£æœ¬åˆ†ä½ˆç‰¹å¾µ**:
   - çˆ†ç™¼æ¬¡æ•¸è¶Šå°‘ï¼Œæ¨£æœ¬æ•¸è¶Šå¤šï¼ˆç¬¦åˆå¸¸æ…‹åˆ†ä½ˆï¼‰
   - çˆ†ç™¼1æ¬¡: 229æª”ï¼ˆæœ€å¤šï¼‰
   - çˆ†ç™¼12æ¬¡: 2æª”ï¼ˆæœ€å°‘ï¼‰

## åˆ†æå•é¡Œ
è«‹ä»¥å°ˆæ¥­é‡åŒ–åˆ†æå¸«çš„è§’åº¦ï¼Œé‡å°ä»¥ä¸Šæ•¸æ“šå›ç­”ä»¥ä¸‹å•é¡Œï¼š

### 1. ç›¸é—œæ€§åˆ†æ
- ã€Œçˆ†ç™¼æ¬¡æ•¸ã€èˆ‡ã€Œå¹³å‡å¹´åº¦æ¼²å¹…ã€ã€ã€Œä¸­ä½æ•¸æ¼²å¹…ã€ã€ã€Œå‹ç‡(>20%)ã€ä¹‹é–“æ˜¯å¦å­˜åœ¨æ­£ç›¸é—œï¼Ÿ
- å¾å“ªäº›æ•¸æ“šé»å¯ä»¥æ”¯æŒä½ çš„çµè«–ï¼Ÿ

### 2. å¹³å‡æ•¸èˆ‡ä¸­ä½æ•¸å·®ç•°åˆ†æ
- å“ªäº›çˆ†ç™¼æ¬¡æ•¸å€é–“çš„ã€Œå¹³å‡-ä¸­ä½æ•¸ã€å·®ç•°æœ€å¤§ï¼Ÿé€™ä»£è¡¨ä»€éº¼æ„ç¾©ï¼Ÿ
- å³å°¾æ•ˆæ‡‰ï¼ˆå¹³å‡>ä¸­ä½ï¼‰æœ€æ˜é¡¯çš„å€é–“æ˜¯å“ªå€‹ï¼Ÿå°æŠ•è³‡ç­–ç•¥æœ‰ä½•å•Ÿç¤ºï¼Ÿ

### 3. æŠ•è³‡ç­–ç•¥å»ºè­°
- æ ¹æ“šæœŸæœ›å€¼ï¼ˆå…¼é¡§æ¨£æœ¬æ•¸èˆ‡æ¼²å¹…ï¼‰ï¼Œå“ªå€‹ã€Œçˆ†ç™¼æ¬¡æ•¸å€é–“ã€æ˜¯æœ€ä½³æŠ•è³‡æ¨™çš„ï¼Ÿ
- å°æ–¼ä¸åŒé¢¨éšªåå¥½çš„æŠ•è³‡è€…ï¼Œä½ æœƒå»ºè­°é—œæ³¨å“ªå€‹çˆ†ç™¼æ¬¡æ•¸å€é–“ï¼Ÿ

### 4. å¸‚å ´è¡Œç‚ºæ´å¯Ÿ
- å¾æ•¸æ“šä¸­ï¼Œä½ èªç‚ºå¸‚å ´å°æ–¼ç‡Ÿæ”¶çˆ†ç™¼çš„ã€Œåæ‡‰æ¨¡å¼ã€æ˜¯ä»€éº¼ï¼Ÿ
- æ˜¯å¦æœ‰ã€Œé‚Šéš›æ•ˆæ‡‰éæ¸›ã€çš„ç¾è±¡ï¼Ÿï¼ˆå³æ›´å¤šæ¬¡çˆ†ç™¼æ˜¯å¦å¸¶ä¾†æ›´é«˜å ±é…¬ï¼Ÿï¼‰

### 5. å¯¦å‹™æ“ä½œå»ºè­°
- æŠ•è³‡äººæ‡‰è©²å¦‚ä½•åˆ©ç”¨é€™å€‹çµ±è¨ˆè¦å¾‹ä¾†åˆ¶å®šäº¤æ˜“ç­–ç•¥ï¼Ÿ
- éœ€è¦æ­é…å“ªäº›å…¶ä»–æŒ‡æ¨™æˆ–æ¢ä»¶ä¾†æé«˜å‹ç‡ï¼Ÿ
"""
    
    col_prompt, col_link = st.columns([2, 1])
    with col_prompt:
        st.write("ğŸ“‹ **AIåˆ†ææŒ‡ä»¤ï¼ˆå·²åŒ…å«å®Œæ•´åƒæ•¸ï¼‰**")
        st.code(prompt_text, language="text", height=500)
        
        # é¡¯ç¤ºåˆ†æé‡é»æ‘˜è¦
        with st.expander("ğŸ” æœ¬æ¬¡åˆ†æé‡é»æ‘˜è¦", expanded=True):
            st.markdown(f"""
            **æ ¸å¿ƒç ”ç©¶å•é¡Œ**ï¼š
            - {metric_name}åœ¨{growth_range[0]}%-{growth_range[1]}%å€é–“
            - {target_year}å¹´å…±{total_stocks}æª”è‚¡ç¥¨ç¬¦åˆæ¢ä»¶
            - åˆ†æçˆ†ç™¼æ¬¡æ•¸èˆ‡å¹´åº¦å ±é…¬çš„é—œä¿‚
            
            **é—œéµç™¼ç¾**ï¼š
            - æœ€é«˜çˆ†ç™¼12æ¬¡ï¼š2æª”ï¼Œå¹³å‡æ¼²å¹…{df_prob.loc[0, 'å¹³å‡å¹´åº¦æ¼²å¹…%'] if len(df_prob) > 0 else 'N/A'}%
            - æœ€ä½çˆ†ç™¼1æ¬¡ï¼š{df_prob.loc[len(df_prob)-1, 'è‚¡ç¥¨æª”æ•¸'] if len(df_prob) > 0 else 'N/A'}æª”ï¼Œå¹³å‡æ¼²å¹…{df_prob.loc[len(df_prob)-1, 'å¹³å‡å¹´åº¦æ¼²å¹…%'] if len(df_prob) > 0 else 'N/A'}%
            - æ¨£æœ¬åˆ†ä½ˆï¼šæ¬¡æ•¸è¶Šå°‘ï¼Œæª”æ•¸è¶Šå¤š
            
            **å¾…è§£å•é¡Œ**ï¼š
            1. æ˜¯å¦å­˜åœ¨æ­£ç›¸é—œï¼Ÿ
            2. æœ€ä½³æŠ•è³‡å€é–“ç‚ºä½•ï¼Ÿ
            3. å¸‚å ´åæ‡‰æ¨¡å¼åˆ†æ
            """)
    
    with col_link:
        st.write("ğŸš€ **AIåˆ†æå¹³å°**")
        encoded_prompt = urllib.parse.quote(prompt_text)
        
        st.link_button(
            "ğŸ”¥ ChatGPT åˆ†æ", 
            f"https://chatgpt.com/?q={encoded_prompt}",
            help="è‡ªå‹•å¸¶å…¥å®Œæ•´åˆ†ææŒ‡ä»¤"
        )
        
        st.link_button(
            "ğŸ” DeepSeek åˆ†æ", 
            "https://chat.deepseek.com/",
            help="è«‹è¤‡è£½ä¸Šæ–¹æŒ‡ä»¤è²¼ä¸Šä½¿ç”¨"
        )
        
        st.link_button(
            "ğŸ¤– Claude åˆ†æ", 
            "https://claude.ai/",
            help="è«‹è¤‡è£½ä¸Šæ–¹æŒ‡ä»¤è²¼ä¸Šä½¿ç”¨"
        )
        
        # å¿«é€Ÿåˆ†ææŒ‰éˆ•
        if st.button("ğŸ“Š åŸ·è¡Œå¿«é€Ÿçµ±è¨ˆåˆ†æ", type="secondary"):
            st.session_state.quick_analysis = True
    
    # å¿«é€Ÿåˆ†æåŠŸèƒ½
    if st.session_state.get("quick_analysis", False):
        st.markdown("### âš¡ å¿«é€Ÿçµ±è¨ˆåˆ†æçµæœ")
        
        # è¨ˆç®—ç›¸é—œä¿‚æ•¸
        numeric_cols = ["å¹³å‡å¹´åº¦æ¼²å¹…%", "ä¸­ä½æ•¸æ¼²å¹…%", "å‹ç‡(>20%)", "ç¿»å€ç‡(>100%)"]
        correlations = {}
        
        for col in numeric_cols:
            if col in df_prob.columns and "çˆ†ç™¼æ¬¡æ•¸" in df_prob.columns:
                corr = df_prob["çˆ†ç™¼æ¬¡æ•¸"].corr(df_prob[col])
                correlations[col] = round(corr, 3)
        
        # é¡¯ç¤ºç›¸é—œä¿‚æ•¸
        cols = st.columns(len(correlations))
        for (col_name, corr_value), col in zip(correlations.items(), cols):
            with col:
                corr_label = "å¼·æ­£ç›¸é—œ" if corr_value > 0.7 else ("ä¸­æ­£ç›¸é—œ" if corr_value > 0.3 else ("å¼±ç›¸é—œ" if corr_value > 0.1 else ("ç„¡ç›¸é—œ" if abs(corr_value) <= 0.1 else ("è² ç›¸é—œ" if corr_value < 0 else "N/A"))))
                st.metric(
                    f"çˆ†ç™¼æ¬¡æ•¸ vs {col_name}",
                    f"{corr_value}",
                    corr_label
                )
        
        # æä¾›ç°¡å–®çµè«–
        st.info(f"""
        **åˆæ­¥è§€å¯Ÿçµè«–**ï¼š
        1. **ç›¸é—œæ€§**: çˆ†ç™¼æ¬¡æ•¸èˆ‡å¹´åº¦å ±é…¬å‘ˆç¾{correlations.get('å¹³å‡å¹´åº¦æ¼²å¹…%', 0) > 0.5 and 'å¼·æ­£ç›¸é—œ' or 'å¼±ç›¸é—œæˆ–ç„¡é—œ'}
        2. **æœ€ä½³å€é–“**: å¾æœŸæœ›å€¼çœ‹ï¼Œçˆ†ç™¼{best_hits if 'best_hits' in locals() else '4-6'}æ¬¡å¯èƒ½æ˜¯æœ€ä½³å€é–“
        3. **é¢¨éšªæç¤º**: é«˜çˆ†ç™¼æ¬¡æ•¸(>10æ¬¡)æ¨£æœ¬éå°‘ï¼Œçµ±è¨ˆæ„ç¾©æœ‰é™
        4. **å³å°¾æ•ˆæ‡‰**: å¹³å‡æ•¸ > ä¸­ä½æ•¸çš„å€é–“æœ‰{pos_diff_count if 'pos_diff_count' in locals() else 'å¤šæ•¸'}å€‹ï¼Œé¡¯ç¤ºå­˜åœ¨å³ååˆ†ä½ˆ
        """)
    
    # ========== E. å‰å¾Œå¹´åº¦æ¯”è¼ƒåˆ†æï¼ˆåŒ…å«ä¸­ä½æ•¸ï¼‰ ==========
    if show_multi_year:
        st.markdown("---")
        st.subheader("ğŸ“ˆ å‰å¾Œå¹´åº¦è¡¨ç¾æ¯”è¼ƒåˆ†æ")
        
        # ç²å–è©³ç´°è‚¡ç¥¨åå–®
        minguo_year = int(target_year) - 1911
        prev_minguo_year = minguo_year - 1
        
        list_query = f"""
        WITH hit_table AS (
            SELECT stock_id, COUNT(*) as hits 
            FROM monthly_revenue 
            WHERE (
                report_month = '{prev_minguo_year}_12' 
                OR (report_month LIKE '{minguo_year}_%' AND report_month <= '{minguo_year}_11')
            )
            AND {study_metric} >= {growth_range[0]} AND {study_metric} < {growth_range[1]}
            GROUP BY stock_id
        )
        SELECT h.stock_id as stock_id, h.hits as hits
        FROM hit_table h
        """
        
        with get_engine().connect() as conn:
            stock_list_df = pd.read_sql_query(text(list_query), conn)
        
        if not stock_list_df.empty:
            # ç²å–å‰å¾Œå¹´åº¦æ•¸æ“š
            multi_year_df = fetch_multi_year_data(stock_list_df['stock_id'].tolist(), target_year)
            
            if not multi_year_df.empty:
                # æŒ‰çˆ†ç™¼æ¬¡æ•¸åˆ†çµ„åˆ†æ
                merged_df = pd.merge(stock_list_df, multi_year_df, on='stock_id')
                
                # è¨ˆç®—å„çˆ†ç™¼æ¬¡æ•¸çš„å‰å¾Œå¹´åº¦è¡¨ç¾ï¼ˆå¹³å‡æ•¸ï¼‰
                year_comparison_mean = merged_df.groupby(['hits', 'year']).agg({
                    'annual_return': 'mean'
                }).unstack().round(1)
                
                # è¨ˆç®—å„çˆ†ç™¼æ¬¡æ•¸çš„å‰å¾Œå¹´åº¦è¡¨ç¾ï¼ˆä¸­ä½æ•¸ï¼‰
                year_comparison_median = merged_df.groupby(['hits', 'year']).agg({
                    'annual_return': 'median'
                }).unstack().round(1)
                
                # é‡æ–°å‘½åæ¬„ä½
                year_comparison_mean.columns = [f'{col[1]}_å¹³å‡' for col in year_comparison_mean.columns]
                year_comparison_median.columns = [f'{col[1]}_ä¸­ä½æ•¸' for col in year_comparison_median.columns]
                
                # åˆä½µå¹³å‡æ•¸å’Œä¸­ä½æ•¸
                year_comparison = pd.concat([year_comparison_mean, year_comparison_median], axis=1)
                
                # é‡æ–°æ’åˆ—åˆ—ï¼Œä½¿æ¯å€‹å¹´åº¦çš„å¹³å‡æ•¸å’Œä¸­ä½æ•¸ç›¸é„°
                # å…ˆç²å–æ‰€æœ‰å”¯ä¸€çš„å¹´ä»½
                years = sorted(set([col.split('_')[0] for col in year_comparison.columns]))
                
                new_order = []
                for year in years:
                    if f'{year}_å¹³å‡' in year_comparison.columns:
                        new_order.append(f'{year}_å¹³å‡')
                    if f'{year}_ä¸­ä½æ•¸' in year_comparison.columns:
                        new_order.append(f'{year}_ä¸­ä½æ•¸')
                
                year_comparison = year_comparison[new_order]
                
                # é‡æ–°å‘½åå¹´ä»½ç‚ºæ›´å®¹æ˜“ç†è§£çš„å½¢å¼
                year_mapping = {
                    str(int(target_year)-2): f'å‰2å¹´({int(target_year)-2})',
                    str(int(target_year)-1): f'å‰1å¹´({int(target_year)-1})',
                    target_year: f'ç›®æ¨™å¹´({target_year})',
                    str(int(target_year)+1): f'å¾Œ1å¹´({int(target_year)+1})'
                }
                
                year_comparison.columns = [year_mapping.get(col.split('_')[0], col.split('_')[0]) + '_' + col.split('_')[1] for col in year_comparison.columns]
                
                st.write("### å‰å¾Œå¹´åº¦å¹³å‡å ±é…¬èˆ‡ä¸­ä½æ•¸æ¯”è¼ƒ (%)")
                st.dataframe(year_comparison.style.format("{:.1f}"), use_container_width=True)
                
                # æ·»åŠ åˆ†æå•é¡Œ
                st.markdown("""
                **å‰å¾Œå¹´åº¦åˆ†æå•é¡Œ**ï¼š
                1. **æå‰åæ‡‰åˆ†æ**ï¼šé«˜çˆ†ç™¼æ¬¡æ•¸çš„è‚¡ç¥¨ï¼Œæ˜¯å¦åœ¨**å‰ä¸€å¹´**å°±å·²ç¶“æœ‰å„ªç•°è¡¨ç¾ï¼Ÿ
                2. **æŒçºŒæ€§åˆ†æ**ï¼šé«˜çˆ†ç™¼æ¬¡æ•¸çš„è‚¡ç¥¨ï¼Œåœ¨**å¾Œä¸€å¹´**æ˜¯å¦ä»ç¶­æŒå¼·å‹¢ï¼Ÿ
                3. **åˆ©å¤šå‡ºç›¡ç¾è±¡**ï¼šæ˜¯å¦å­˜åœ¨ç›®æ¨™å¹´å¤§æ¼²ï¼Œä½†å¾Œä¸€å¹´ä¸‹è·Œçš„æƒ…æ³ï¼Ÿ
                4. **ä¸­ä½æ•¸ vs å¹³å‡æ•¸**ï¼šå“ªäº›å¹´åº¦/çˆ†ç™¼æ¬¡æ•¸çš„ã€Œå¹³å‡æ•¸èˆ‡ä¸­ä½æ•¸å·®ç•°ã€æœ€å¤§ï¼Ÿä»£è¡¨ä»€éº¼ï¼Ÿ
                """)
                
                # æ·»åŠ ç°¡å–®æ´å¯Ÿ
                if not year_comparison.empty:
                    # æ‰¾å‡ºæå‰åæ‡‰æœ€æ˜é¡¯çš„å€é–“
                    early_response = {}
                    for hits in year_comparison.index:
                        target_year_col = f'ç›®æ¨™å¹´({target_year})_å¹³å‡'
                        prev_year_col = f'å‰1å¹´({int(target_year)-1})_å¹³å‡'
                        
                        if target_year_col in year_comparison.columns and prev_year_col in year_comparison.columns:
                            target_return = year_comparison.loc[hits, target_year_col]
                            prev_return = year_comparison.loc[hits, prev_year_col]
                            
                            if pd.notna(target_return) and pd.notna(prev_return):
                                early_response[hits] = prev_return
                    
                    if early_response:
                        max_hits = max(early_response, key=early_response.get)
                        max_return = early_response[max_hits]
                        
                        st.info(f"""
                        **æ´å¯Ÿç™¼ç¾**ï¼š
                        - **æå‰åæ‡‰æœ€æ˜é¡¯**ï¼šçˆ†ç™¼{max_hits}æ¬¡çš„è‚¡ç¥¨ï¼Œå‰ä¸€å¹´å¹³å‡æ¼²å¹…{max_return:.1f}%
                        - **æŠ•è³‡å•Ÿç¤º**ï¼šå¦‚æœå‰ä¸€å¹´å·²å¤§æ¼²ï¼Œå¯èƒ½å·²åæ˜ éƒ¨åˆ†é æœŸï¼Œéœ€æ³¨æ„è¿½é«˜é¢¨éšª
                        """)
    
    # ========== F. å€é–“åå–®é»ååŠŸèƒ½ï¼ˆä¿®æ­£ä¸‹è¼‰æŒ‰éˆ•éŒ¯èª¤ï¼‰ ==========
    st.markdown("---")
    st.subheader("ğŸ” è©³ç´°åå–®åˆ†æ")
    
    hit_options = df_prob["çˆ†ç™¼æ¬¡æ•¸"].tolist()
    selected_hits = st.selectbox("é¸æ“‡ã€çˆ†ç™¼æ¬¡æ•¸ã€æŸ¥çœ‹å…·é«”è‚¡ç¥¨åå–®ï¼š", hit_options, key="hits_selector")
    
    # ç²å–è©³ç´°åå–®
    detail_query = f"""
    WITH hit_table AS (
        SELECT stock_id, COUNT(*) as hits 
        FROM monthly_revenue 
        WHERE (
            report_month = '{prev_minguo_year}_12' 
            OR (report_month LIKE '{minguo_year}_%' AND report_month <= '{minguo_year}_11')
        )
        AND {study_metric} >= {growth_range[0]} AND {study_metric} < {growth_range[1]}
        GROUP BY stock_id
    )
    SELECT h.stock_id as "è‚¡ç¥¨ä»£è™Ÿ", 
           COALESCE(m.stock_name, 'N/A') as "è‚¡ç¥¨åç¨±",
           h.hits as "çˆ†ç™¼æ¬¡æ•¸",
           ROUND(((k.year_close - k.year_open)/k.year_open*100)::numeric, 1) as "å¹´åº¦æ¼²å¹…%",
           ROUND(AVG(m.{study_metric})::numeric, 1) as "å¹³å‡å¢é•·%",
           STRING_AGG(DISTINCT CASE WHEN m.remark <> '-' AND m.remark <> '' THEN m.remark END, ' | ') as "é—œéµå‚™è¨»"
    FROM hit_table h
    LEFT JOIN stock_annual_k k ON h.stock_id = SPLIT_PART(k.symbol, '.', 1) AND k.year = '{target_year}'
    LEFT JOIN monthly_revenue m ON h.stock_id = m.stock_id 
      AND (m.report_month LIKE '{minguo_year}_%' OR m.report_month = '{prev_minguo_year}_12')
    WHERE h.hits = {selected_hits}
    GROUP BY h.stock_id, m.stock_name, k.year_close, k.year_open, h.hits
    ORDER BY "å¹´åº¦æ¼²å¹…%" DESC NULLS LAST;
    """
    
    with get_engine().connect() as conn:
        detail_df = pd.read_sql_query(text(detail_query), conn)
    
    if not detail_df.empty:
        st.write(f"### ğŸ† {target_year}å¹´ã€ç‡Ÿæ”¶çˆ†ç™¼ {selected_hits} æ¬¡ã€è‚¡ç¥¨æ¸…å–®ï¼ˆå…±{len(detail_df)}æª”ï¼‰")
        
        # åå–®çµ±è¨ˆ
        if len(detail_df) > 0:
            avg_return = detail_df["å¹´åº¦æ¼²å¹…%"].mean()
            median_return = detail_df["å¹´åº¦æ¼²å¹…%"].median()
            positive_count = (detail_df["å¹´åº¦æ¼²å¹…%"] > 0).sum()
            positive_rate = positive_count / len(detail_df) * 100
            
            col_s1, col_s2, col_s3, col_s4 = st.columns(4)
            col_s1.metric("å¹³å‡å¹´åº¦æ¼²å¹…", f"{avg_return:.1f}%")
            col_s2.metric("ä¸­ä½æ•¸æ¼²å¹…", f"{median_return:.1f}%")
            col_s3.metric("ä¸Šæ¼²æª”æ•¸", f"{positive_count}æª”")
            col_s4.metric("ä¸Šæ¼²æ¯”ä¾‹", f"{positive_rate:.1f}%")
        
        st.dataframe(detail_df, use_container_width=True)
        
        # åå–®å°ˆå±¬AIåˆ†æ
        st.markdown("### ğŸ¤– åå–®æ·±åº¦è¨ºæ–·")
        
        # å»ºæ§‹åå–®Markdown
        l_header = "| " + " | ".join(detail_df.columns) + " |"
        l_sep = "| " + " | ".join(["---"] * len(detail_df.columns)) + " |"
        l_rows = ["| " + " | ".join(map(str, r.values)) + " |" for _, r in detail_df.iterrows()]
        list_md = "\n".join([l_header, l_sep] + l_rows)

        list_prompt = f"""
# {target_year}å¹´ç‡Ÿæ”¶çˆ†ç™¼{selected_hits}æ¬¡è‚¡ç¥¨è©³ç´°åˆ†æ

## åˆ†æèƒŒæ™¯
- **ç›®æ¨™å¹´åº¦**: {target_year}å¹´
- **çˆ†ç™¼æ¬¡æ•¸**: {selected_hits}æ¬¡
- **å¢é•·æŒ‡æ¨™**: {metric_name}
- **é–€æª»ç¯„åœ**: {growth_range[0]}% è‡³ {growth_range[1]}%
- **æ¨£æœ¬æ•¸é‡**: {len(detail_df)}æª”è‚¡ç¥¨

## è©³ç´°åå–®æ•¸æ“š
{list_md}

## åå–®çµ±è¨ˆæ‘˜è¦
- å¹³å‡å¹´åº¦æ¼²å¹…: {avg_return:.1f}%
- ä¸­ä½æ•¸æ¼²å¹…: {median_return:.1f}%
- ä¸Šæ¼²è‚¡ç¥¨æ¯”ä¾‹: {positive_rate:.1f}%
- æœ€é«˜æ¼²å¹…: {detail_df['å¹´åº¦æ¼²å¹…%'].max():.1f}%
- æœ€ä½æ¼²å¹…: {detail_df['å¹´åº¦æ¼²å¹…%'].min():.1f}%

## åˆ†æå•é¡Œ
è«‹é‡å°é€™ä»½åå–®é€²è¡Œæ·±åº¦åˆ†æï¼š

1. **ç”¢æ¥­ç‰¹å¾µåˆ†æ**ï¼š
   - å¾ã€Œé—œéµå‚™è¨»ã€æ¬„ä½ä¸­ï¼Œé€™äº›è‚¡ç¥¨æ˜¯å¦æœ‰å…±åŒçš„ç”¢æ¥­ç‰¹æ€§ï¼Ÿ
   - æ˜¯å¦å­˜åœ¨æŸç¨®ã€Œç‡Ÿæ”¶èªåˆ—æ¨¡å¼ã€ï¼Ÿï¼ˆå¦‚ï¼šå°ˆæ¡ˆå…¥å¸³ã€å­£ç¯€æ€§å› ç´ ç­‰ï¼‰

2. **è¡¨ç¾å·®ç•°è§£è®€**ï¼š
   - ç‚ºä»€éº¼æœ‰äº›è‚¡ç¥¨ã€Œå¹³å‡å¢é•·%ã€å¾ˆé«˜ï¼Œä½†ã€Œå¹´åº¦æ¼²å¹…%ã€å»ä¸çªå‡ºï¼Ÿ
   - ä»¥8476å°å¢ƒç‚ºä¾‹ï¼Œå¦‚æœæ•¸æ“šä¸­å­˜åœ¨ï¼Œè«‹åˆ†æå…¶é«˜å¢é•·ä½†ä½æ¼²å¹…çš„åŸå› 

3. **å¹³å‡æ•¸èˆ‡ä¸­ä½æ•¸åˆ†æ**ï¼š
   - å¹³å‡æ¼²å¹…({avg_return:.1f}%) vs ä¸­ä½æ•¸æ¼²å¹…({median_return:.1f}%)çš„å·®ç•°ä»£è¡¨ä»€éº¼ï¼Ÿ
   - æ˜¯å¦å­˜åœ¨å³å°¾æ•ˆæ‡‰ï¼ˆå°‘æ•¸è‚¡ç¥¨æ¼²å¹…æ¥µé«˜ï¼Œæ‹‰é«˜å¹³å‡ï¼‰ï¼Ÿ

4. **æŠ•è³‡å•Ÿç¤º**ï¼š
   - å¾é€™ä»½åå–®ä¸­ï¼ŒæŠ•è³‡äººæ‡‰è©²æ³¨æ„å“ªäº›é—œéµæŒ‡æ¨™ï¼Ÿ
   - å¦‚ä½•å€åˆ†ã€ŒçœŸæˆé•·ã€èˆ‡ã€Œä¸€æ¬¡æ€§å¢é•·ã€ï¼Ÿ

5. **ç­–ç•¥å»ºè­°**ï¼š
   - å°æ–¼çˆ†ç™¼{selected_hits}æ¬¡çš„è‚¡ç¥¨ï¼Œæœ€ä½³çš„è²·è³£æ™‚æ©Ÿç‚ºä½•ï¼Ÿ
   - éœ€è¦æ­é…å“ªäº›æŠ€è¡“æŒ‡æ¨™æˆ–åŸºæœ¬é¢æ¢ä»¶ä¾†æé«˜å‹ç‡ï¼Ÿ
"""
        
        col_lp, col_ll = st.columns([2, 1])
        with col_lp:
            st.code(list_prompt, language="text", height=400)
        with col_ll:
            encoded_list_p = urllib.parse.quote(list_prompt)
            st.link_button("ğŸ”¥ ChatGPT åˆ†æåå–®", f"https://chatgpt.com/?q={encoded_list_p}")
            st.link_button("ğŸ” DeepSeek åˆ†æ", "https://chat.deepseek.com/")
            
            # ä¿®æ­£ï¼šä½¿ç”¨ st.download_button è€Œä¸æ˜¯ st.link_button
            st.download_button(
                label="ğŸ“Š ä¸‹è¼‰åå–®CSV",
                data=detail_df.to_csv(index=False).encode('utf-8'),
                file_name=f'burst_{selected_hits}_stocks_{target_year}.csv',
                mime='text/csv'
            )

else:
    st.warning(f"âš ï¸ åœ¨ {target_year} å¹´åŠè¨­å®šæ¢ä»¶ä¸‹ï¼Œæ²’æœ‰ç¬¦åˆæ¢ä»¶çš„æ¨£æœ¬ã€‚")
    st.info("""
    ğŸ’¡ **èª¿æ•´å»ºè­°**ï¼š
    1. é™ä½çˆ†ç™¼é–€æª»å€¼
    2. æ›´æ›åˆ†æå¹´åº¦  
    3. å˜—è©¦ä¸åŒçš„å¢é•·æŒ‡æ¨™
    4. æ”¾å¯¬å¢é•·ç¯„åœ
    """)

# ========== 7. é å°¾è³‡è¨Š ==========
st.markdown("---")
footer_col1, footer_col2, footer_col3 = st.columns(3)
with footer_col1:
    st.markdown("**ç‰ˆæœ¬**ï¼šæ©Ÿç‡ç ”ç©¶å®¤ 2.0")
with footer_col2:
    st.markdown(f"**æ•¸æ“šé€±æœŸ**ï¼š{int(target_year)-2 if show_multi_year else 2019}-{int(target_year)+1 if show_multi_year else 2025}")
with footer_col3:
    st.markdown("**ç ”ç©¶é‡é»**ï¼šçˆ†ç™¼æ¬¡æ•¸ vs å¹´åº¦å ±é…¬")

# åˆå§‹åŒ–session state
if 'quick_analysis' not in st.session_state:
    st.session_state.quick_analysis = False
